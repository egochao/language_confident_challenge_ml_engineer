{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0494e2bf",
   "metadata": {},
   "source": [
    "### Hi, let check out this dataset\n",
    "\n",
    "#### Here is what I gonna do \n",
    "\n",
    "1. Check the dataset description from source - Just get more information, I dont trust them. lol\n",
    "2. Listen to the data by myself => Avoid me making some dumb assumptions\n",
    "3. Check label\n",
    "4. Plot some audios in raw and spectrogram form, etc => To think of a good representation for this\n",
    "5. Verify some basic assumption about train - test - val\n",
    "6. Can we reduce the input size\n",
    "7. How we going to train this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac52ceb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import random\n",
    "from IPython.display import Audio, display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "from utils.visualize_utils import log_specgram, plot_audio, plot_fft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580efc6d",
   "metadata": {},
   "source": [
    "### Dataset description -- SpeechCommands V1\n",
    "#### References\n",
    "- [Official release](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html)  \n",
    "- [Huggning face description](https://huggingface.co/datasets/speech_commands)\n",
    "- [Torch dataset api](https://pytorch.org/audio/stable/datasets.html#speechcommands)\n",
    "\n",
    "\n",
    "#### Extracted info\n",
    "1. Data set created by thousands of different people, contributed by members of the public through the AIY website\n",
    "2. The dataset has 65,000 one-second long utterances of 30 short words\n",
    "3. Dataset is in English\n",
    "4. Dataset include some common words(yes, no, etc)\n",
    "5. There are unknown class contain random words\n",
    "6. There are silence class contain only background noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceef529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33.3%"
     ]
    }
   ],
   "source": [
    "# download dataset\n",
    "_ = SPEECHCOMMANDS(\"./data\", download=True, url='speech_commands_v0.01')\n",
    "source_data_path = Path('data/SpeechCommands/speech_commands_v0.01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05223246",
   "metadata": {},
   "source": [
    "### Let's listen to some audio samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1d90df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audio_files = list(source_data_path.glob('**/*.wav'))\n",
    "audio_widgets = []\n",
    "for file in random.sample(audio_files, 10):\n",
    "    sample_rate, audio = wavfile.read(file)\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "        display(Audio(data=audio, rate=sample_rate))\n",
    "    audio_widgets.append(out)\n",
    "widgets.HBox(audio_widgets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f3f14",
   "metadata": {},
   "source": [
    "#### What we get\n",
    "\n",
    "1. Multiple recording device was used\n",
    "2. Audio generally clean - but a about 20% audio I check have background noise\n",
    "3. The audio part is not cover all 1 seconds sometime there are 50% of audio is silence\n",
    "4. There are some mic artifact in the audio - sound like people turn the mic on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cf5c4a",
   "metadata": {},
   "source": [
    "### Label exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "count_labels = []\n",
    "for path in source_data_path.iterdir():\n",
    "    if path.is_dir():\n",
    "        labels.append(path.name)\n",
    "        count = len(list(path.glob('*.wav')))\n",
    "        count_labels.append(count)\n",
    " \n",
    "# creating the bar plot\n",
    "fig = plt.figure(figsize = (17, 5))\n",
    "plt.bar(labels, count_labels, color ='maroon',\n",
    "        width = 0.9)\n",
    "plt.xticks(rotation=90)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.xlabel(\"Label count\")\n",
    "plt.ylabel(\"Number of recording\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde89e08",
   "metadata": {},
   "source": [
    "#### What we get\n",
    "1. Dataset quite balace with most of class have 1500-2300 samples\n",
    "2. The silence class only have 6 files\n",
    "3. I also do random sampling to check for mislabel data => I dont found any mis-label samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ed616",
   "metadata": {},
   "source": [
    "### Check train - test - val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d6366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset structure\n",
    "# waveform, sample_rate, label, speaker_id, utterance_number\n",
    "train_ds = SPEECHCOMMANDS(\"./data\", subset=\"training\", url='speech_commands_v0.01')\n",
    "test_ds = SPEECHCOMMANDS(\"./data\", subset=\"testing\", url='speech_commands_v0.01')\n",
    "val_ds = SPEECHCOMMANDS(\"./data\", subset=\"validation\", url='speech_commands_v0.01')\n",
    "print(\"train set: \", len(train_ds))\n",
    "print(\"test set: \", len(test_ds))\n",
    "print(\"val set: \", len(val_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc42de1f",
   "metadata": {},
   "source": [
    "#### Do train - test - val set have overlap speakers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3bb06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_speakers = [data[3] for data in train_ds]\n",
    "test_set_speakers = [data[3] for data in test_ds]\n",
    "val_set_speakers = [data[3] for data in val_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6bb090",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(train_set_speakers).intersection(test_set_speakers))\n",
    "print(set(train_set_speakers).intersection(val_set_speakers))\n",
    "print(set(test_set_speakers).intersection(val_set_speakers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5925ff",
   "metadata": {},
   "source": [
    "#### what we get\n",
    "1. The train - test - val is divided 80-10-10% => which is fine\n",
    "2. There are no overlap speakers between train - test - val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3522f0ef",
   "metadata": {},
   "source": [
    "### Visualize wave and spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c70d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/SpeechCommands/speech_commands_v0.01/cat/0ab3b47d_nohash_1.wav'\n",
    "plot_audio(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/SpeechCommands/speech_commands_v0.01/no/0cd323ec_nohash_0.wav'\n",
    "plot_audio(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c43472b",
   "metadata": {},
   "source": [
    "#### What we get \n",
    "1. The audio contain some redundant audio - match with what I hear\n",
    "2. There are some perfect silence in some audio - which can be an editing artifact => I dont think It would affect training much \n",
    "3. The duration of the audio array from dataset is not uniform ~=16000 => We have to use padding if we choose model with fix size input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c4a1c",
   "metadata": {},
   "source": [
    "### Can we reduce the size of the input?\n",
    "#### Let do some fft to find upper frequency bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263cb346",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fft('data/SpeechCommands/speech_commands_v0.01/no/0cd323ec_nohash_0.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecce555",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fft('data/SpeechCommands/speech_commands_v0.01/cat/0ab3b47d_nohash_1.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0cafe2",
   "metadata": {},
   "source": [
    "#### Most of frequencies in the data are well below 4000hz\n",
    "=> From some others source and my judgment let try to resample to reduce input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3313622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_rate = 8000\n",
    "\n",
    "sample_rate, samples = wavfile.read('data/SpeechCommands/speech_commands_v0.01/cat/0ab3b47d_nohash_1.wav')\n",
    "resampled = signal.resample(samples, int(new_sample_rate/sample_rate * samples.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e070bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(samples, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(resampled, rate=new_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c8689",
   "metadata": {},
   "source": [
    "#### What we get\n",
    "1. From FFT we know that most frequencies in the data are lower than 4000hz\n",
    "2. Resample audio from 16000hz -> 8000hz dont change the audio much, I still can hear the audio clearly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e1478",
   "metadata": {},
   "source": [
    "### How we going to train this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd7095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addfbf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b98405c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
